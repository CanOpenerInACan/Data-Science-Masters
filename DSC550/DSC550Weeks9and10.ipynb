{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #1: Sklearn Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the dataframe\n",
    "comments = pd.read_json('categorized-comments.jsonl', lines = True, nrows = 606475)\n",
    "\n",
    "#Reduce the size of the dataframe. Our model will learn most with higher sample, but want it to run quickly\n",
    "comments_small = comments.sample(n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-a78645ab1712>:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  comments_small.Tokens = comments_small.Tokens.str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "#Let's make a new column to tokenize and clean our comments\n",
    "comments_small['Tokens'] = comments_small.txt.astype(str)\n",
    "\n",
    "#Lowercase everything so we don't have some words appearing twice and occurences being split\n",
    "comments_small.Tokens = comments_small.Tokens.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "#Remove all punctuation\n",
    "comments_small.Tokens = comments_small.Tokens.str.replace('[^\\w\\s]','')\n",
    "\n",
    "#Remove our stop words that don't hold much value\n",
    "stop = stopwords.words('english')\n",
    "comments_small.Tokens = comments_small.Tokens.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "#Stem our words to help combine them\n",
    "stemmer = PorterStemmer()\n",
    "comments_small.Tokens = comments_small.Tokens.apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets find the 3000 most important words across the comments\n",
    "tfidf = TfidfVectorizer(max_features = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to use our tokens column to predict our category column\n",
    "X = tfidf.fit_transform(comments_small.Tokens)\n",
    "y = comments_small.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split it into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import our classifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get our model settings right\n",
    "nn_mod1 = MLPClassifier(hidden_layer_sizes = [500,150], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.82078696\n",
      "Iteration 2, loss = 0.62236762\n",
      "Iteration 3, loss = 0.44570094\n",
      "Iteration 4, loss = 0.30256657\n",
      "Iteration 5, loss = 0.22431673\n",
      "Iteration 6, loss = 0.17455883\n",
      "Iteration 7, loss = 0.14169894\n",
      "Iteration 8, loss = 0.11608851\n",
      "Iteration 9, loss = 0.09819927\n",
      "Iteration 10, loss = 0.08991280\n",
      "Iteration 11, loss = 0.08622183\n",
      "Iteration 12, loss = 0.08354546\n",
      "Iteration 13, loss = 0.08206702\n",
      "Iteration 14, loss = 0.08139388\n",
      "Iteration 15, loss = 0.08191729\n",
      "Iteration 16, loss = 0.08004866\n",
      "Iteration 17, loss = 0.07785579\n",
      "Iteration 18, loss = 0.07932246\n",
      "Iteration 19, loss = 0.07854181\n",
      "Iteration 20, loss = 0.08096869\n",
      "Iteration 21, loss = 0.07608846\n",
      "Iteration 22, loss = 0.07807805\n",
      "Iteration 23, loss = 0.07737410\n",
      "Iteration 24, loss = 0.07825016\n",
      "Iteration 25, loss = 0.07681095\n",
      "Iteration 26, loss = 0.07662739\n",
      "Iteration 27, loss = 0.07732100\n",
      "Iteration 28, loss = 0.07729130\n",
      "Iteration 29, loss = 0.07781927\n",
      "Iteration 30, loss = 0.07759858\n",
      "Iteration 31, loss = 0.07828681\n",
      "Iteration 32, loss = 0.07792202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "#Now let's use the training data (X_train, y_train) to predict our category.\n",
    "target_predicted = nn_mod1.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check out what we got\n",
    "matrix = confusion_matrix(y_test, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting ready to show the confusion matrix\n",
    "class_names = ['Science and Technology', 'Sports', 'Video Games']\n",
    "dataframe = pd.DataFrame(matrix, columns=class_names, index=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#And we can graph it. Not amazing but not bad\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(dataframe, annot=True, cbar=None, cmap='Blues', fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "Science and Technology       0.60      0.25      0.35        48\n",
      "                Sports       0.62      0.50      0.55       237\n",
      "           Video Games       0.81      0.90      0.85       715\n",
      "\n",
      "              accuracy                           0.77      1000\n",
      "             macro avg       0.68      0.55      0.59      1000\n",
      "          weighted avg       0.76      0.77      0.76      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We didn't have many science and technology comments in our data, so it was hard to teach the model to catch it. \n",
    "#The video games section works out pretty well though. \n",
    "\n",
    "print(classification_report(y_test,\n",
    "                           target_predicted, \n",
    "                           target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from sklearn.datasets import make_multilabel_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3000) (5000, 3)\n"
     ]
    }
   ],
   "source": [
    "#Set up the variables we need, and make multilabel classification\n",
    "\n",
    "N_FEATURES = 3000\n",
    "N_CLASSES=3\n",
    "\n",
    "X, y = make_multilabel_classification(n_samples=5000, n_features=3000, n_classes=3, n_labels=3, random_state=1)\n",
    "# summarize dataset shape\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make our model\n",
    "nn=Sequential()\n",
    "nn.add(Dense(500, activation='relu', input_shape=(N_FEATURES,)))\n",
    "nn.add(Dense(150, activation='relu'))\n",
    "nn.add(Dense(N_CLASSES, activation='sigmoid'))\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.5382 - accuracy: 0.2438\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.2910 - accuracy: 0.6285\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.0993 - accuracy: 0.6285\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.0188 - accuracy: 0.6380 0s - loss: 0.0191 - accuracy: 0.63\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.0042 - accuracy: 0.6180\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.0019 - accuracy: 0.5993\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.0012 - accuracy: 0.5982\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 8.3481e-04 - accuracy: 0.5895\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 6.2727e-04 - accuracy: 0.5980 0s - loss: 6.2727e-04 - accuracy: 0.59\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 4.8917e-04 - accuracy: 0.5845\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9365e-04 - accuracy: 0.58 - 1s 24ms/step - loss: 3.9393e-04 - accuracy: 0.5875\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 3.2359e-04 - accuracy: 0.5863\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.7084e-04 - accuracy: 0.5838\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 2.3008e-04 - accuracy: 0.5820\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.9808e-04 - accuracy: 0.5845\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.7213e-04 - accuracy: 0.5780\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.5109e-04 - accuracy: 0.5792\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.3367e-04 - accuracy: 0.5810\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 1.1902e-04 - accuracy: 0.5795\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.0676e-04 - accuracy: 0.5780\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 9.6217e-05 - accuracy: 0.5755 0s - loss: 1.0249e-0\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 8.7099e-05 - accuracy: 0.5788 0s - loss: 8.7739e-05 - accu\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 7.9182e-05 - accuracy: 0.5792\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 7.2376e-05 - accuracy: 0.5753\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 6.6307e-05 - accuracy: 0.5763\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 6.1012e-05 - accuracy: 0.5748\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 5.6320e-05 - accuracy: 0.5750\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.2089e-05 - accuracy: 0.5745 0s - loss: 5.2654e-05 - accuracy: \n",
      "Epoch 29/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 4.8354e-05 - accuracy: 0.5720\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 4.4998e-05 - accuracy: 0.5702\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 4.1957e-05 - accuracy: 0.5730\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 3.9187e-05 - accuracy: 0.5715\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 3.6692e-05 - accuracy: 0.5710\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 3.4442e-05 - accuracy: 0.5713 0s - loss: 3.6047e-05 \n",
      "Epoch 35/200\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 3.2353e-05 - accuracy: 0.5720\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 3.0452e-05 - accuracy: 0.5715\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 2.8707e-05 - accuracy: 0.5700\n",
      "Epoch 38/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 2.7105e-05 - accuracy: 0.5713 0s - loss: 2.7650e\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.5623e-05 - accuracy: 0.5707\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 2.4270e-05 - accuracy: 0.5710\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 2.2998e-05 - accuracy: 0.5702\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 2.1817e-05 - accuracy: 0.5710 0s - loss: 2.1841e-05 - ac\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 2.0725e-05 - accuracy: 0.5685\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.9713e-05 - accuracy: 0.5673\n",
      "Epoch 45/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.8791e-05 - accuracy: 0.56 - 1s 24ms/step - loss: 1.8770e-05 - accuracy: 0.5695\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.7888e-05 - accuracy: 0.5702\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.7057e-05 - accuracy: 0.5705 0s - loss: 1.7124e-05 - accuracy: \n",
      "Epoch 48/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.6289e-05 - accuracy: 0.5695\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.5567e-05 - accuracy: 0.5663\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.4885e-05 - accuracy: 0.5667 0s - loss: 1.4773e-05 - accuracy: 0.55 - ETA: 0s - loss: 1.4722e-05 - accuracy: \n",
      "Epoch 51/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.4246e-05 - accuracy: 0.5655\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.3638e-05 - accuracy: 0.5648\n",
      "Epoch 53/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.3072e-05 - accuracy: 0.5648\n",
      "Epoch 54/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.2538e-05 - accuracy: 0.5660\n",
      "Epoch 55/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.2032e-05 - accuracy: 0.5655 0s - loss: 1.230\n",
      "Epoch 56/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.1554e-05 - accuracy: 0.5675\n",
      "Epoch 57/200\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 1.1102e-05 - accuracy: 0.5630\n",
      "Epoch 58/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0675e-05 - accuracy: 0.5640\n",
      "Epoch 59/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0266e-05 - accuracy: 0.5648\n",
      "Epoch 60/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 9.8777e-06 - accuracy: 0.5640\n",
      "Epoch 61/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 9.5097e-06 - accuracy: 0.5635\n",
      "Epoch 62/200\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 9.1631e-06 - accuracy: 0.5633 0s - loss: 9.1895e-0\n",
      "Epoch 63/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 8.8332e-06 - accuracy: 0.5642\n",
      "Epoch 64/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 8.5177e-06 - accuracy: 0.5633 0s - loss: 8.4993e-06 - accuracy: 0.56\n",
      "Epoch 65/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 8.2181e-06 - accuracy: 0.5627\n",
      "Epoch 66/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 7.9338e-06 - accuracy: 0.5642 0s - loss: 8.2298e-06 \n",
      "Epoch 67/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 7.6590e-06 - accuracy: 0.5627\n",
      "Epoch 68/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 7.3972e-06 - accuracy: 0.5627 0s - loss: 7.4771e-06 - accuracy\n",
      "Epoch 69/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 7.1488e-06 - accuracy: 0.5625\n",
      "Epoch 70/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 6.9102e-06 - accuracy: 0.5625\n",
      "Epoch 71/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 6.6847e-06 - accuracy: 0.5615\n",
      "Epoch 72/200\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 6.4686e-06 - accuracy: 0.5630\n",
      "Epoch 73/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 6.2590e-06 - accuracy: 0.5620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 6.0575e-06 - accuracy: 0.5623\n",
      "Epoch 75/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.8666e-06 - accuracy: 0.5615\n",
      "Epoch 76/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 5.6834e-06 - accuracy: 0.5623\n",
      "Epoch 77/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 5.5064e-06 - accuracy: 0.5630\n",
      "Epoch 78/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 5.3368e-06 - accuracy: 0.5633\n",
      "Epoch 79/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.1748e-06 - accuracy: 0.5633\n",
      "Epoch 80/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 5.0196e-06 - accuracy: 0.5627 0s - loss: 4.9528e-06 - accura\n",
      "Epoch 81/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 4.8693e-06 - accuracy: 0.5625\n",
      "Epoch 82/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.7248e-06 - accuracy: 0.5623\n",
      "Epoch 83/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.5857e-06 - accuracy: 0.5625\n",
      "Epoch 84/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.4519e-06 - accuracy: 0.5627\n",
      "Epoch 85/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 4.3230e-06 - accuracy: 0.5623\n",
      "Epoch 86/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.1991e-06 - accuracy: 0.5605 0s - loss: 4.1991e-06 - accuracy: 0.56\n",
      "Epoch 87/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.0790e-06 - accuracy: 0.5600\n",
      "Epoch 88/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 3.9642e-06 - accuracy: 0.5610\n",
      "Epoch 89/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 3.8534e-06 - accuracy: 0.5617\n",
      "Epoch 90/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 3.7460e-06 - accuracy: 0.5608\n",
      "Epoch 91/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 3.6432e-06 - accuracy: 0.5602\n",
      "Epoch 92/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 3.5431e-06 - accuracy: 0.5617 0s - loss: 3.5191e-06 - accuracy: 0.\n",
      "Epoch 93/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 3.4471e-06 - accuracy: 0.5615\n",
      "Epoch 94/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 3.3550e-06 - accuracy: 0.5612\n",
      "Epoch 95/200\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 3.2647e-06 - accuracy: 0.5612\n",
      "Epoch 96/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 3.1775e-06 - accuracy: 0.5612\n",
      "Epoch 97/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 3.0935e-06 - accuracy: 0.5608\n",
      "Epoch 98/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 3.0121e-06 - accuracy: 0.5612\n",
      "Epoch 99/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 2.9335e-06 - accuracy: 0.5617 0s - loss: 2.9515e-06 - ac\n",
      "Epoch 100/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 2.8575e-06 - accuracy: 0.5605\n",
      "Epoch 101/200\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 2.7841e-06 - accuracy: 0.5610\n",
      "Epoch 102/200\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 2.7128e-06 - accuracy: 0.5610\n",
      "Epoch 103/200\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 2.6445e-06 - accuracy: 0.5590\n",
      "Epoch 104/200\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 2.5776e-06 - accuracy: 0.5615\n",
      "Epoch 105/200\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 2.5127e-06 - accuracy: 0.5612 0s - loss: 2.5192e-06 \n",
      "Epoch 106/200\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 2.4498e-06 - accuracy: 0.5617\n",
      "Epoch 107/200\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 2.3886e-06 - accuracy: 0.5615\n",
      "Epoch 108/200\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 2.3296e-06 - accuracy: 0.5605\n",
      "Epoch 109/200\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 2.2715e-06 - accuracy: 0.5592\n",
      "Epoch 110/200\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 2.2159e-06 - accuracy: 0.5605\n",
      "Epoch 111/200\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 2.1621e-06 - accuracy: 0.5600\n",
      "Epoch 112/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 2.1093e-06 - accuracy: 0.5600\n",
      "Epoch 113/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.0584e-06 - accuracy: 0.5600\n",
      "Epoch 114/200\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 2.0089e-06 - accuracy: 0.5598 0s - loss: 2.0296e-06 - ac\n",
      "Epoch 115/200\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.9607e-06 - accuracy: 0.5598\n",
      "Epoch 116/200\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 1.9142e-06 - accuracy: 0.5600\n",
      "Epoch 117/200\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.8690e-06 - accuracy: 0.5600\n",
      "Epoch 118/200\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1.8250e-06 - accuracy: 0.5592\n",
      "Epoch 119/200\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1.7824e-06 - accuracy: 0.5587\n",
      "Epoch 120/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.7407e-06 - accuracy: 0.5583 0s - loss: 1.7429e-06 - accuracy\n",
      "Epoch 121/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.7005e-06 - accuracy: 0.5598\n",
      "Epoch 122/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.6610e-06 - accuracy: 0.5587\n",
      "Epoch 123/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.6229e-06 - accuracy: 0.5590 0s - loss: 1.6352e-06 - accu\n",
      "Epoch 124/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.5853e-06 - accuracy: 0.5587\n",
      "Epoch 125/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.5491e-06 - accuracy: 0.5590 0s - loss: 1.5906e-06 - accuracy: 0. - ETA: 0s - loss: 1.613\n",
      "Epoch 126/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.5138e-06 - accuracy: 0.5583\n",
      "Epoch 127/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.4796e-06 - accuracy: 0.5595\n",
      "Epoch 128/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.4461e-06 - accuracy: 0.5590\n",
      "Epoch 129/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.4135e-06 - accuracy: 0.5592\n",
      "Epoch 130/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.3821e-06 - accuracy: 0.5580\n",
      "Epoch 131/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.3514e-06 - accuracy: 0.5580\n",
      "Epoch 132/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.3212e-06 - accuracy: 0.5587 0s - loss: 1.3048e-06 - accuracy: \n",
      "Epoch 133/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.2922e-06 - accuracy: 0.5585 0s - loss: 1.3324e-06 - accura\n",
      "Epoch 134/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.2636e-06 - accuracy: 0.5583 0s - loss: 1.2946e-06 - \n",
      "Epoch 135/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.2359e-06 - accuracy: 0.5583 0s - loss: 1.2359e-06 - accuracy: 0.55\n",
      "Epoch 136/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.2091e-06 - accuracy: 0.5590\n",
      "Epoch 137/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.1827e-06 - accuracy: 0.5575\n",
      "Epoch 138/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.1569e-06 - accuracy: 0.5580\n",
      "Epoch 139/200\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.1319e-06 - accuracy: 0.5583\n",
      "Epoch 140/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.1076e-06 - accuracy: 0.5573\n",
      "Epoch 141/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0836e-06 - accuracy: 0.5570\n",
      "Epoch 142/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0603e-06 - accuracy: 0.5577 0s - loss: 1.0592e-06 - accuracy: 0.55\n",
      "Epoch 143/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0373e-06 - accuracy: 0.5577\n",
      "Epoch 144/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0153e-06 - accuracy: 0.5577\n",
      "Epoch 145/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 9.9385e-07 - accuracy: 0.5575 0s - loss: 9.9461e-07 - accuracy: 0.\n",
      "Epoch 146/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 9.7275e-07 - accuracy: 0.5577 0s - loss: 9.7460e-07 - accuracy: 0.\n",
      "Epoch 147/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 9.5223e-07 - accuracy: 0.5577\n",
      "Epoch 148/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 9.3220e-07 - accuracy: 0.5577\n",
      "Epoch 149/200\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 9.1259e-07 - accuracy: 0.5580 0s - loss: 8.8567e-07 \n",
      "Epoch 150/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 8.9352e-07 - accuracy: 0.5580 0s - loss: 8.7066e-07 - ac\n",
      "Epoch 151/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 8.7504e-07 - accuracy: 0.5590\n",
      "Epoch 152/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 8.5686e-07 - accuracy: 0.5573\n",
      "Epoch 153/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 8.3917e-07 - accuracy: 0.5575\n",
      "Epoch 154/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 8.2170e-07 - accuracy: 0.5575 0s - loss: 8.3891e-07 - accura - ETA: 0s - loss: 8.2115e-07 - accuracy: 0.55\n",
      "Epoch 155/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 8.0469e-07 - accuracy: 0.5570\n",
      "Epoch 156/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 7.8811e-07 - accuracy: 0.5570\n",
      "Epoch 157/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 7.7198e-07 - accuracy: 0.5567\n",
      "Epoch 158/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 7.5617e-07 - accuracy: 0.5577\n",
      "Epoch 159/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 7.4076e-07 - accuracy: 0.5573 0s - loss: 7.3685e-07 - ac\n",
      "Epoch 160/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 7.2568e-07 - accuracy: 0.5565\n",
      "Epoch 161/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 7.1092e-07 - accuracy: 0.5567\n",
      "Epoch 162/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 6.9653e-07 - accuracy: 0.5573\n",
      "Epoch 163/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 6.8242e-07 - accuracy: 0.5573\n",
      "Epoch 164/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 6.6850e-07 - accuracy: 0.5565\n",
      "Epoch 165/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 6.5519e-07 - accuracy: 0.5558\n",
      "Epoch 166/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 6.4208e-07 - accuracy: 0.5558 0s - loss: 6.3\n",
      "Epoch 167/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 6.2936e-07 - accuracy: 0.5555\n",
      "Epoch 168/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 6.1697e-07 - accuracy: 0.5567\n",
      "Epoch 169/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 6.0461e-07 - accuracy: 0.5567\n",
      "Epoch 170/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 5.9261e-07 - accuracy: 0.5573 0s - loss: 6.0224e-07 - \n",
      "Epoch 171/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.8088e-07 - accuracy: 0.5567 0s - loss: 5.7529e-07 - accura\n",
      "Epoch 172/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.6941e-07 - accuracy: 0.5558\n",
      "Epoch 173/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 5.5819e-07 - accuracy: 0.5567\n",
      "Epoch 174/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.4720e-07 - accuracy: 0.5565\n",
      "Epoch 175/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.3650e-07 - accuracy: 0.5560\n",
      "Epoch 176/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.2604e-07 - accuracy: 0.5573\n",
      "Epoch 177/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.1578e-07 - accuracy: 0.5558\n",
      "Epoch 178/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 5.0572e-07 - accuracy: 0.5560\n",
      "Epoch 179/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.9586e-07 - accuracy: 0.5562\n",
      "Epoch 180/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.8626e-07 - accuracy: 0.5558\n",
      "Epoch 181/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.7676e-07 - accuracy: 0.5570\n",
      "Epoch 182/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 4.6753e-07 - accuracy: 0.5565\n",
      "Epoch 183/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 4.5854e-07 - accuracy: 0.5548\n",
      "Epoch 184/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 4.4968e-07 - accuracy: 0.5545\n",
      "Epoch 185/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 4.4103e-07 - accuracy: 0.5548\n",
      "Epoch 186/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 4.3260e-07 - accuracy: 0.5560\n",
      "Epoch 187/200\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 4.2434e-07 - accuracy: 0.5567\n",
      "Epoch 188/200\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 4.1617e-07 - accuracy: 0.5552\n",
      "Epoch 189/200\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 4.0822e-07 - accuracy: 0.5560\n",
      "Epoch 190/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 4.0047e-07 - accuracy: 0.5550\n",
      "Epoch 191/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 3.9294e-07 - accuracy: 0.5552\n",
      "Epoch 192/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 3.8543e-07 - accuracy: 0.5543\n",
      "Epoch 193/200\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 3.7822e-07 - accuracy: 0.5548 0s - loss: 3.8074e-07 - accuracy: \n",
      "Epoch 194/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 3.7104e-07 - accuracy: 0.5548\n",
      "Epoch 195/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 3.6410e-07 - accuracy: 0.5550\n",
      "Epoch 196/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 3.5721e-07 - accuracy: 0.5545\n",
      "Epoch 197/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 3.5051e-07 - accuracy: 0.5543\n",
      "Epoch 198/200\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 3.4396e-07 - accuracy: 0.5543 0s - loss: 3.5170e-07 - accu\n",
      "Epoch 199/200\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 3.3755e-07 - accuracy: 0.5545\n",
      "Epoch 200/200\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 3.3130e-07 - accuracy: 0.5550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1558fae8cd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model to the training data\n",
    "nn.fit(X_train, y_train, epochs=200, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the test data\n",
    "predictions = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round the numbers so we can compare\n",
    "predictions = predictions.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "Science and Technology       0.86      0.94      0.90       787\n",
      "                Sports       0.86      0.97      0.91       824\n",
      "           Video Games       0.57      0.44      0.50       344\n",
      "\n",
      "             micro avg       0.82      0.86      0.84      1955\n",
      "             macro avg       0.76      0.78      0.77      1955\n",
      "          weighted avg       0.81      0.86      0.83      1955\n",
      "           samples avg       0.82      0.81      0.79      1955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\anaconda3\\envs\\Testing\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\David\\anaconda3\\envs\\Testing\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Print the classification report. It has been giving me errors here, may need to start up the next section after \n",
    "#Running this\n",
    "print(classification_report(y_test,\n",
    "                           predictions, \n",
    "                           target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #3: Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=1\n",
    "height=28\n",
    "width=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.reshape(data_train.shape[0], height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.reshape(data_test.shape[0], height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = data_train / 255\n",
    "features_test = data_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the neural network\n",
    "network = Sequential()\n",
    "\n",
    "#Add a convolutional layer with 64 filters, a 5x5 window and a ReLU activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "                 kernel_size=(5,5),\n",
    "                 input_shape=(width, height, channels),\n",
    "                 activation='relu'))\n",
    "\n",
    "#Add a max pooling layer with 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Add a dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "#Add a layer to flatten input\n",
    "network.add(Flatten())\n",
    "\n",
    "#Add a fully connected layer with 128 units and ReLU activation function\n",
    "network.add(Dense(128, activation='relu'))\n",
    "\n",
    "#Add a dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "#add a fully connected layer with softmax activation function\n",
    "network.add(Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "network.compile(loss='categorical_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60/60 [==============================] - 64s 1s/step - loss: 0.5833 - accuracy: 0.8202\n",
      "Epoch 2/2\n",
      "60/60 [==============================] - 62s 1s/step - loss: 0.1893 - accuracy: 0.9444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1558dc6ea90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the neural network\n",
    "network.fit(features_train,\n",
    "           target_train,\n",
    "           epochs=2,\n",
    "           verbose=1,\n",
    "           batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x155945475b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANTElEQVR4nO3dYahc9ZnH8d9vTRsxDZK7IdlLGjY1Cq4EN9UgimFVSmM2IrGoS0JYUpXevqjQYl9UVKioCyLbLPvGwC1K06WbUjRiqKWthLiub0puJNVr77bGkDZpLokxhCYSqOY+++KeyDXeOXMzc86cuXm+H7jMzHnmnPNw9JdzZv4z83dECMDF72+abgBAbxB2IAnCDiRB2IEkCDuQxJxe7sw2b/0DNYsIT7e8qzO77bW2f297v+2Hu9kWgHq503F225dI+oOkr0o6LGmPpI0R8buSdTizAzWr48x+g6T9EXEgIv4q6aeS1nexPQA16ibsSyQdmvL4cLHsU2wP2R6xPdLFvgB0qZs36Ka7VPjMZXpEDEsalriMB5rUzZn9sKSlUx5/UdKR7toBUJduwr5H0lW2v2T785I2SNpZTVsAqtbxZXxEfGz7QUm/knSJpOcj4p3KOgNQqY6H3jraGa/ZgdrV8qEaALMHYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0l0PGUzeue6664rre/YsaNlbdmyZRV30z/WrFlTWh8bG2tZO3ToUNXt9L2uwm77oKRTks5K+jgiVlXRFIDqVXFmvy0ijlewHQA14jU7kES3YQ9Jv7a91/bQdE+wPWR7xPZIl/sC0IVuL+NvjogjthdJetX2/0XE61OfEBHDkoYlyXZ0uT8AHerqzB4RR4rbY5JeknRDFU0BqF7HYbc9z/b8c/clrZE0WlVjAKrVzWX8Ykkv2T63nf+OiF9W0hU+5fbbby+tz507t0ed9Jc777yztH7//fe3rG3YsKHqdvpex2GPiAOS/rHCXgDUiKE3IAnCDiRB2IEkCDuQBGEHkuArrn1gzpzy/wzr1q3rUSezy969e0vrDz30UMvavHnzStf98MMPO+qpn3FmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGfvA7fddltp/aabbiqtP/PMM1W2M2ssWLCgtH7NNde0rF122WWl6zLODmDWIuxAEoQdSIKwA0kQdiAJwg4kQdiBJBzRu0lass4Is2LFitL6a6+9Vlr/4IMPSuvXX399y9rp06dL153N2h231atXt6wNDg6Wrvv+++930lJfiAhPt5wzO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwffZe+Cxxx4rrbf7DfO1a9eW1i/WsfSBgYHS+i233FJan5iYqLKdWa/tmd3287aP2R6dsmzA9qu23y1uy39FAEDjZnIZ/yNJ559aHpa0KyKukrSreAygj7UNe0S8LunEeYvXS9pW3N8m6a5q2wJQtU5fsy+OiHFJiohx24taPdH2kKShDvcDoCK1v0EXEcOShqW8X4QB+kGnQ29HbQ9KUnF7rLqWANSh07DvlLS5uL9Z0svVtAOgLm0v421vl3SrpIW2D0v6vqSnJf3M9gOS/iTp3jqb7Hf33HNPab3d/Or79+8vrY+MjFxwTxeDRx99tLTebhy97PvuJ0+e7KCj2a1t2CNiY4vSVyruBUCN+LgskARhB5Ig7EAShB1IgrADSfAV1wrce2/5yGO76YGfffbZKtuZNZYtW1Za37RpU2n97NmzpfWnnnqqZe2jjz4qXfdixJkdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnH2GLr/88pa1G2+8sattb926tav1Z6uhofJfK1u4cGFpfWxsrLS+e/fuC+7pYsaZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9hubOnduytmTJktJ1t2/fXnU7F4Xly5d3tf7o6Gj7J+ETnNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2Wfo1KlTLWv79u0rXffaa68trQ8MDJTWT5w4UVrvZ4sWLWpZazfVdTtvvPFGV+tn0/bMbvt528dsj05Z9rjtP9veV/yVT0AOoHEzuYz/kaS10yz/j4hYWfz9otq2AFStbdgj4nVJs/c6EoCk7t6ge9D2W8Vl/oJWT7I9ZHvE9kgX+wLQpU7DvlXSckkrJY1L+kGrJ0bEcESsiohVHe4LQAU6CntEHI2IsxExIemHkm6oti0AVeso7LYHpzz8miS+awj0ubbj7La3S7pV0kLbhyV9X9KttldKCkkHJX2zvhb7w5kzZ1rW3nvvvdJ177777tL6K6+8UlrfsmVLab1OK1asKK1fccUVpfWyOdgjopOWPjExMdHV+tm0DXtEbJxm8XM19AKgRnxcFkiCsANJEHYgCcIOJEHYgSTc7fDHBe3M7t3Oeujqq68urT/xxBOl9TvuuKO0XvYz1nU7fvx4ab3d/z9l0y7b7qinc+bPn19aLxsuvZhFxLQHljM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOHsfWLlyZWn9yiuv7E0j03jhhRe6Wn/btm0ta5s2bepq23Pm8Evo02GcHUiOsANJEHYgCcIOJEHYgSQIO5AEYQeSYKCyD7Sb8rldvZ8dOHCgtm23+5nr0VGmM5iKMzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4O2pV9tvw3f5uPOPoF6btmd32Utu7bY/Zfsf2t4vlA7Zftf1ucbug/nYBdGoml/EfS/puRPyDpBslfcv2NZIelrQrIq6StKt4DKBPtQ17RIxHxJvF/VOSxiQtkbRe0rnfHNom6a6aegRQgQt6zW57maQvS/qNpMURMS5N/oNge1GLdYYkDXXZJ4AuzTjstr8g6UVJ34mIv8z0zZWIGJY0XGyDH5wEGjKjoTfbn9Nk0H8SETuKxUdtDxb1QUnH6mkRQBVm8m68JT0naSwitkwp7ZS0ubi/WdLL1beH2S4iavvDhZnJZfzNkv5V0tu29xXLHpH0tKSf2X5A0p8k3VtLhwAq0TbsEfGGpFYv0L9SbTsA6sLHZYEkCDuQBGEHkiDsQBKEHUiCr7iiVpdeemnH6545c6bCTsCZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJwdtbrvvvta1k6ePFm67pNPPllxN7lxZgeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnR6327NnTsrZly5aWNUnavXt31e2kxpkdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Jwu3mubS+V9GNJfydpQtJwRPyn7cclfUPS+8VTH4mIX7TZFpNqAzWLiGlnXZ5J2AclDUbEm7bnS9or6S5J/yLpdET8+0ybIOxA/VqFfSbzs49LGi/un7I9JmlJte0BqNsFvWa3vUzSlyX9plj0oO23bD9ve0GLdYZsj9ge6a5VAN1oexn/yRPtL0j6H0n/FhE7bC+WdFxSSHpSk5f697fZBpfxQM06fs0uSbY/J+nnkn4VEZ/59kJxxv95RKxosx3CDtSsVdjbXsbbtqTnJI1NDXrxxt05X5M02m2TAOozk3fjV0v6X0lva3LoTZIekbRR0kpNXsYflPTN4s28sm1xZgdq1tVlfFUIO1C/ji/jAVwcCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0n0esrm45L+OOXxwmJZP+rX3vq1L4neOlVlb3/fqtDT77N/Zuf2SESsaqyBEv3aW7/2JdFbp3rVG5fxQBKEHUii6bAPN7z/Mv3aW7/2JdFbp3rSW6Ov2QH0TtNndgA9QtiBJBoJu+21tn9ve7/th5vooRXbB22/bXtf0/PTFXPoHbM9OmXZgO1Xbb9b3E47x15DvT1u+8/Fsdtne11DvS21vdv2mO13bH+7WN7osSvpqyfHreev2W1fIukPkr4q6bCkPZI2RsTvetpIC7YPSloVEY1/AMP2P0k6LenH56bWsv2MpBMR8XTxD+WCiPhen/T2uC5wGu+aems1zfjX1eCxq3L68040cWa/QdL+iDgQEX+V9FNJ6xvoo+9FxOuSTpy3eL2kbcX9bZr8n6XnWvTWFyJiPCLeLO6fknRumvFGj11JXz3RRNiXSDo05fFh9dd87yHp17b32h5quplpLD43zVZxu6jhfs7XdhrvXjpvmvG+OXadTH/erSbCPt3UNP00/ndzRFwn6Z8lfau4XMXMbJW0XJNzAI5L+kGTzRTTjL8o6TsR8Zcme5lqmr56ctyaCPthSUunPP6ipCMN9DGtiDhS3B6T9JImX3b0k6PnZtAtbo813M8nIuJoRJyNiAlJP1SDx66YZvxFST+JiB3F4saP3XR99eq4NRH2PZKusv0l25+XtEHSzgb6+Azb84o3TmR7nqQ16r+pqHdK2lzc3yzp5QZ7+ZR+mca71TTjavjYNT79eUT0/E/SOk2+I/+epEeb6KFFX1dI+m3x907TvUnarsnLuo80eUX0gKS/lbRL0rvF7UAf9fZfmpza+y1NBmuwod5Wa/Kl4VuS9hV/65o+diV99eS48XFZIAk+QQckQdiBJAg7kARhB5Ig7EAShB1IgrADSfw/gg1DYNhx/+sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's try to predict this image. We should get 4\n",
    "\n",
    "plt.imshow(features_test[4], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets get the image in the right dimensions for our model\n",
    "img = features_test[4]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting the dimensions to be right. It should be number of images, height, width, channels\n",
    "img = np.expand_dims(img,0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5135172e-05, 2.6792259e-05, 8.0592734e-05, 7.7359609e-06,\n",
       "        9.9648958e-01, 1.2645596e-05, 2.0100904e-04, 1.5404330e-04,\n",
       "        7.2383475e-05, 2.9400436e-03]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's predict the number. All of them should be small except for the 5th (0,1,2,3 all before 4) which is close to 1\n",
    "network.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
